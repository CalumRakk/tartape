# ADR-001: Deterministic Ordering

## Status
Accepted

## Context

Unlike tools that generate a TAR file and save it to disk, `tartape` is a **streaming generation** engine. This means the TAR file does not physically exist anywhere; it is built byte-by-byte in memory while being transmitted.

In this scenario, determinism is the only guarantee of consistency. If a transmission is interrupted (for example, while uploading volume 10 of a total of 50) and we need to resume it, the engine must be able to **regenerate the exact same sequence of bytes** it produced in the first attempt.

If the file order is not deterministic (because the operating system returns files in random orders), the "virtual tape" generated by `tartape` changes its structure in each execution. This causes:
1.  **Offset Inconsistency:** The byte `X` where the upload failed no longer corresponds to the same fragment of information, invalidating any attempt to resume.
2.  **Volume Corruption:** Already uploaded fragments will not fit with the new ones, generating a corrupt final file.
3.  **Hash Variation:** The file identifier (Hash) will change in each attempt even if the source data remains the same, making it impossible to verify the integrity of the total stream.

## Decision
`tartape` will implement determinism as a structural requirement to guarantee that a stream is **reproducible and resumable**:

1.  **Sequence Guarantee:** The engine will ensure that the processing order of the files is identical in each execution. By default, this will be achieved through alphabetical sorting during folder discovery (`add_folder`).
2.  **T0 State Persistence:** The engine will allow working with a "frozen" file list (injected externally). This ensures that if a streaming process needs to be restarted days later, the engine uses the exact same sequence defined in the initial inventory, regardless of how the operating system sees the files at that moment.

## Consequences

*   **Positive:**
    *  **Stream Resumption:** Allows resuming interrupted transmissions with byte precision, as the engine can reconstruct the "tape" exactly as it was.
    *  **Volume Consistency:** Guarantees that the chunks of a file (volumes) are mathematically consistent with each other at the end of the process.
    *  **Verifiability:** The Hash of the resulting flow will always be the same for a given set of files, allowing for reliable integrity audits.

*   **Negative:**
    *  **Inventory Overhead:** To guarantee order, the engine must know or list the files before emitting the first byte, which introduces a small initial latency compared to purely random streaming.
    *  **Scanning Memory Usage:** Sorting massive file structures requires temporarily storing the names in memory before starting the generation.
